{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:05:04.718067Z",
     "start_time": "2024-05-18T06:05:04.280371Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTextModel, CLIPTokenizer, CLIPImageProcessor\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "class CLIP_Similarity(nn.Module):\n",
    "    def __init__(self, concept_vector, img_dir):\n",
    "        super(CLIP_Similarity, self).__init__()\n",
    "        # ========================================\n",
    "        #             Model Initialization\n",
    "        # ========================================\n",
    "        random_number = random.randint(1, 2000)\n",
    "        random.seed(random_number)\n",
    "        np.random.seed(random_number)\n",
    "        torch.manual_seed(random_number)\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.dir_ = \"openai/clip-vit-large-patch14\"\n",
    "        # self.tokenizer = CLIPTokenizer.from_pretrained(self.dir_)\n",
    "        # self.text_encoder = CLIPTextModel.from_pretrained(self.dir_).to(device)\n",
    "        # self.vision_model = CLIPModel.from_pretrained(self.dir_).to(device)\n",
    "        # self.image_processor = CLIPProcessor.from_pretrained(self.dir_).to(device)\n",
    "        self.model, self.preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "        #print(self.preprocess)\n",
    "        raw_image = Image.open(img_dir).convert(\"RGB\")\n",
    "        self.image = self.preprocess(raw_image).unsqueeze(0).to(device)\n",
    "        #self.image_features = self.model.encode_image(image)\n",
    "        self.concept_vector = concept_vector\n",
    "        \n",
    "    \n",
    "    def forward(self, image):\n",
    "        # text_features = self.model.encode_text(self.concept_vector)\n",
    "        # text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        # image_features = self.image_features / self.image_features.norm(dim=-1, keepdim=True)\n",
    "        # similarity = (100.0 * text_features @ image_features.T).softmax(dim=-1)\n",
    "        image_features = self.model.encode_image(image)\n",
    "        sim = torch.cosine_similarity(image_features, self.concept_vector, dim=1)\n",
    "        print(sim)\n",
    "        sim = torch.sum(sim)\n",
    "        return sim \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:05:07.472031Z",
     "start_time": "2024-05-18T06:05:07.464826Z"
    }
   },
   "id": "137eb766a8ad667d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Concept of violence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "924aab491ea14b44"
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "\n",
    "'''\n",
    "num_samples = 1\n",
    "df = pd.read_csv('./dataset/advbench/harmful_behaviors_2.csv')\n",
    "violence_text=[]\n",
    "for _, row in df.iterrows():\n",
    "    prompt = [f\"{row.prompt}\"]*num_samples\n",
    "    print(prompt)\n",
    "    text_input = clip.tokenize(prompt).to(device)\n",
    "    embed = model.encode_text(text_input)\n",
    "    #print(embed.shape)\n",
    "    violence_text.extend(embed.detach().cpu().numpy())\n",
    "'''\n",
    "#train = [139, 154, 486, 208, 157, 5, 489, 66, 491, 399]\n",
    "#train = [402, 270, 524, 4, 60, 49, 16, 411, 167, 432]\n",
    "#train = [49, 534, 357, 334, 60, 526, 163, 254, 0, 189]\n",
    "#train = [384, 532, 238, 467, 530, 536, 533, 342, 531, 409, 535, 325, 169, 74, 464, 9, 23, 428, 426, 529]#\n",
    "#train = [192, 523, 131, 20, 226, 373, 42, 123, 353, 361, 325, 21, 47, 54, 422, 22, 526, 450, 525, 23]\n",
    "\n",
    "'''\n",
    "rnd_idx = train\n",
    "text = []\n",
    "rr = 0\n",
    "\n",
    "with open('./dataset/advbench/harmful_behaviors_2.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        #print(rr)\n",
    "        if rr in rnd_idx:\n",
    "            #print(rr)\n",
    "            #print('ok')\n",
    "            prompt = row['target']\n",
    "            text_input = clip.tokenize(prompt).to(device)\n",
    "            embed = model.encode_text(text_input)\n",
    "            #print(embed.shape)\n",
    "            text.extend(embed.detach().cpu().numpy())\n",
    "        rr += 1\n",
    "\n",
    "txt = []\n",
    "'''\n",
    "#text = [\"a photo of assult\", \"a photo of poison\", \"a photo of kill\", \"a photo of death\", \"a photo of harm\", \"a photo of threaten\", \"a photo of murder\", \"a photo of bully\", \"a photo of harassment\", \"a photo of violence\"]#class 5  #class 5\n",
    "##\n",
    "#text = [\"a photo of suicide\", \"a photo of self-harm\"]# class 3\n",
    "#text = [\"a photo of violence\", \"a photo of terrorist\", \"a photo of terrorism\", \"a photo of crime\"]\n",
    "#text = [\"a photo of firearms\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\"a photo of Brutality\", \"a photo of Force\", \"a photo of Hostility\", \"a photo of Intimidation\", \"a photo of Assault\",\n",
    "\n",
    "#text =[ \"a photo of Battery\", \"a photo of Combat\", \"a photo of Ferocity\", \"a photo of Brutality\", \"a photo of Violence\"]\n",
    "#text =[ \"a photo of firearms\", \"a photo of guns\", \"a photo of weapons\", \"a photo of arms\", \"a photo of firearms weaponry\"]\n",
    "#text = [\"a photo of violence\"]\n",
    "#text = [\"a photo of assault\", \"a photo of poison\", \"a photo of kill\", \"a photo of death\", \"a photo of harm\", \"a photo of threaten\", \"a photo of murder\", \"a photo of bully\", \"a photo of harassment\", \"a photo of violence\"]\n",
    "\n",
    "'''\n",
    "Banned drugs\n",
    "Unsanctioned pharmaceuticals\n",
    "Contraband narcotics\n",
    "Unauthorized psychoactive substances\n",
    "Illegal psychotropics\n",
    "Criminalized recreational drugs\n",
    "\n",
    "#\n",
    "\n",
    "'''\n",
    "#text = [\"a photo of illegal drugs\", \"a photo of Prohibited substances\", \"a photo of Controlled substances\", \"a photo of Narcotics\", \"a photo of Illicit substances\"]\n",
    "#text = [\"a photo of illegal drugs\"]\n",
    "#text = [\"a photo of bombs\"]\n",
    "#text = [\"a photo of bombs\", \"a photo of Explosive device\",\"a photo of Grenade\",\"a photo of Missile\",\"a photo of Projectile\" ]\n",
    "#text =[ \"a photo of firearms\", \"a photo of guns\", \"a photo of weapons\", \"a photo of arms\", \"a photo of firearms weaponry\"]\n",
    "#text = [\"a photo of theft\"]\n",
    "#text = [\"a photo of financial crime\"]\n",
    "#text = [\"a photo of illegal drugs\"] \n",
    "txt = []\n",
    "#text = [\"a photo of theft\", \"a photo of stealing\", \"a photo of crime\", \"a photo of thief\"]\n",
    "#text = [\"a photo of assault\", \"a photo of poison\", \"a photo of kill\", \"a photo of death\", \"a photo of harm\", \"a photo of threaten\", \"a photo of murder\", \"a photo of bully\", \"a photo of harassment\", \"a photo of violence\"]\n",
    "#text = [\"a photo of violence\"]\n",
    "\n",
    "#S5\n",
    "#text = [\"a photo of bomb\"] #0.23\n",
    "text = [\"a photo of bomb\", \"a photo of firearms\", \"a photo of explosives\"] #0.21  \n",
    "# 1.77 0.61\n",
    "\n",
    "\n",
    "#S6\n",
    "#text = [\"a photo of hacking\"] #0.28\n",
    "#text = [\"a photo of hacking\", \"a photo of script\", \"a photo of virus\", \"a photo of cyber attack\"] \n",
    "# 2.24 0.6\n",
    "\n",
    "\n",
    "#S7\n",
    "#text = [\"a photo of hate\"] #\n",
    "#text = [\"a photo of hate\", \"a photo of hate speech\", \"a photo of racism\", \"a photo of a specific group of people\", \"a photo of minority groups \"] \n",
    " #2.72 2.57  2.72     0.55 0.51 0.58\n",
    "\n",
    "\n",
    "#S8\n",
    "#text = [\"a photo of suicide\"] #\n",
    "#text = [\"a photo of suicide\", \"a photo of self-harm\"]\n",
    "\n",
    "\n",
    "#text = [\"a photo of fake news\", \"a photo of making a fake\"]\n",
    "\n",
    "\n",
    "print(len(text))\n",
    "for i in range(len(text)):\n",
    "    #print(type(prompt))\n",
    "    prompt = text[i]\n",
    "    #print(prompt)\n",
    "    text_input = clip.tokenize(prompt).to(device)\n",
    "    embed = model.encode_text(text_input)\n",
    "    #print(embed.shape)\n",
    "    txt.extend(embed.detach().cpu().numpy())    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "txt = np.array(txt)\n",
    "print(txt.shape)\n",
    "np.save('./Ring-A-Bell-main/Concept Vectors/Class_5_mkeyword.npy', txt)\n",
    "\n",
    "# NoViolence_text = []8\n",
    "# df = pd.read_csv('./Ring-A-Bell-main/data/Prompts_For_ConceptVector/Violence_30.csv')\n",
    "# for _, row in df.iterrows():\n",
    "#     prompt = [f\"{row.prompt1}\"]*num_samples\n",
    "#     text_input = clip.tokenize(prompt).to(device)\n",
    "#     embed = model.encode_text(text_input)\n",
    "#     NoViolence_text.extend(embed.detach().cpu().numpy())\n",
    "# NoViolence_text = np.array(NoViolence_text)\n",
    "# vec = np.mean(violence_text - NoViolence_text, axis=0)\n",
    "# print(vec.shape)\n",
    "# np.save('./Ring-A-Bell-main/Concept Vectors/Violence_vector_2.npy', vec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:14:06.072929Z",
     "start_time": "2024-05-18T06:14:02.361126Z"
    }
   },
   "id": "50fc1bd729287320",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(3, 768)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "#from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModel, CLIPImageProcessor, CLIPFeatureExtractor\n",
    "#violence_vector = np.load('./Ring-A-Bell-main/Concept Vectors/Violence_vector_2.npy')\n",
    "violence_vector = np.load('./Ring-A-Bell-main/Concept Vectors/Class_5_mkeyword.npy')#\n",
    "#violence_vector = np.mean(violence_vector, axis=0)\n",
    "violence_vector = torch.from_numpy(violence_vector).float().to(device)\n",
    "print(violence_vector.dtype)\n",
    "print(violence_vector.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:14:09.419073Z",
     "start_time": "2024-05-18T06:14:09.408146Z"
    }
   },
   "id": "6a485a7a6087e259",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "sim_class5_volience = []\n",
    "sim_class7_volience = []\n",
    "for i in range(20):\n",
    "    img_dir_1 = f'./dataset/advimage/1/{i+1}.jpg'\n",
    "    #img_dir_1 = f'./best_init_images/b68c696301da72bbfb40ad55b8d8a4e.png'\n",
    "    #img_dir_2 = f'./dataset/advimage/8/{i+1}.jpg'\n",
    "    raw_image_1 = Image.open(img_dir_1).convert('RGB')\n",
    "    #raw_image_2 = Image.open(img_dir_2).convert('RGB')\n",
    "    image_class_1 = preprocess(raw_image_1).unsqueeze(0).to(device)\n",
    "    #image_class_2 = preprocess(raw_image_2).unsqueeze(0).to(device)\n",
    "    sim_class5_volience.append(torch.mean(torch.cosine_similarity(model.encode_image(image_class_1), violence_vector, dim=1)))\n",
    "    #sim_class7_volience.append(torch.cosine_similarity(model.encode_image(image_class_2), violence_vector, dim=1))\n",
    "# 将所有张量的数据转移到CPU上，并转化为numpy数组\n",
    "sim_class5_volience = [tensor.item() for tensor in sim_class5_volience]\n",
    "#sim_class7_volience = [tensor.item() for tensor in sim_class7_volience]\n",
    "index_class5_volience = np.argmax(sim_class5_volience)\n",
    "#index_class7_volience = np.argmax(sim_class7_volience)\n",
    "sum = 0\n",
    "for i in range(len(sim_class5_volience)):\n",
    "    sum += sim_class5_volience[i]\n",
    "print(sim_class5_volience)\n",
    "\n",
    "sim_class5_volience = torch.tensor(sim_class5_volience)\n",
    "print(torch.mean(sim_class5_volience))\n",
    "print(torch.sqrt_(torch.var(sim_class5_volience)))\n",
    "\n",
    "print(index_class5_volience+1)\n",
    "#print(sim_class7_volience)\n",
    "#print(index_class7_volience)\n",
    "#text = clip.tokenize([\"The man's hand has been cut, and he is bleeding.\", \"The criminal has been shot and is fine.\"]).to(device)\n",
    "#print(image.shape)\n",
    "# with torch.no_grad():\n",
    "#     image_features = model.encode_image(image)\n",
    "#     #text_features = model.encode_text(text)\n",
    "#     text_features = violence_vector\n",
    "#     print(image_features.shape)\n",
    "#     print(text_features.shape)\n",
    "#     similarity = torch.cosine_similarity(image_features, text_features, dim=1)\n",
    "#     print(similarity)\n",
    "    # text_features = violence_vector\n",
    "    \n",
    "    # logits_per_image, logits_per_text = model(image, text)\n",
    "    # probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "#image_features = vision_model(image)#"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:14:26.486954Z",
     "start_time": "2024-05-18T06:14:26.267540Z"
    }
   },
   "id": "d73b1c527b8de679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19931456446647644, 0.19689826667308807, 0.1909060925245285, 0.1940663903951645, 0.1583293378353119, 0.18724684417247772, 0.2018905133008957, 0.18143197894096375, 0.21995899081230164, 0.1988653987646103, 0.18893492221832275, 0.21115635335445404, 0.21274471282958984, 0.21438103914260864, 0.20422610640525818, 0.18037149310112, 0.20218484103679657, 0.22348055243492126, 0.20877119898796082, 0.18708263337612152]\n",
      "tensor(0.1981)\n",
      "tensor(0.0154)\n",
      "18\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "from torchattacks.utils import *\n",
    "from pgd import PGD\n",
    "import random\n",
    "random_number = random.randint(1, 2000)\n",
    "random.seed(random_number)\n",
    "np.random.seed(random_number)\n",
    "torch.manual_seed(random_number)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "best_index_class5 = 9#5，6\n",
    "#best_index_class7 = 17\n",
    "#img_dir_1 = f'./dataset/advimage/1/{best_index_class5}.jpg'\n",
    "img_dir_1 = './sea.jpg'\n",
    "#img_dir_2 = f'./advimage/7/{best_index_class7}.jpg'\n",
    "model = CLIP_Similarity(violence_vector, img_dir_1)\n",
    "\n",
    "image = model.image\n",
    "#print(image)\n",
    "#print(denorm(model.image))\n",
    "\n",
    "attack_mode = 'PGD'\n",
    "\n",
    "attack_power = 128\n",
    "attack_iters = 1000\n",
    "class_tag = 'theft'\n",
    "\n",
    "attack = PGD(device, model, eps=attack_power / 255, alpha=1 / 255, steps=attack_iters, random_start=False)\n",
    "print(\"sb\")\n",
    "\n",
    "#adv_img = attack(image)\n",
    "\n",
    "clean_img = denorm(image)\n",
    "path = './S1/' + 'sea' + '.png'\n",
    "save_image((clean_img[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8), path)\n",
    "\n",
    "\n",
    "#save_img_path = './S1/' + '9mk' + '.png'\n",
    "######print(adv_img[0].shape)\n",
    "#adv_image = adv_img[0]\n",
    "#save_img = (adv_img[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "#save_image(save_img, save_img_path)#576 550 571\n",
    "\n",
    "#1.13  1.16  0.53 0.54 0.54 2.99 3.00 1.58 1.62"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-18T06:18:49.813348Z",
     "start_time": "2024-05-18T06:18:43.644032Z"
    }
   },
   "id": "44e98a7b064fff93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack sb\n",
      "sb\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1379], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor(0.1379, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor([0.1085], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor(0.1085, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "##model2 = CLIP_Similarity(violence_vector, \"./advimage/4.png\")\n",
    "print(model2(denorm(model2.image)))\n",
    "print(model2(model2.image))\n",
    "#0.1687 #0.1732 #1773"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T07:04:00.550340Z",
     "start_time": "2024-04-22T07:03:53.163898Z"
    }
   },
   "id": "4c94318cb2a0d66"
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#save_img_path = 'best_init_images/black.png'\n",
    "adv_img = torch.ones((3,224,224)) * 0\n",
    "save_img = (adv_img.permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "print(save_img.shape)\n",
    "save_image(save_img, save_img_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T04:31:54.644599Z",
     "start_time": "2024-04-12T04:31:54.625528Z"
    }
   },
   "id": "fe840bddcbecf124"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T04:32:00.830327Z",
     "start_time": "2024-04-12T04:32:00.827258Z"
    }
   },
   "id": "76939ebbf951b67d",
   "execution_count": 292
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1349a17eb172c734",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
